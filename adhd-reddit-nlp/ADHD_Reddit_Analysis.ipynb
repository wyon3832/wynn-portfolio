{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9Hv7w_cJi6o"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQvCxVlkkh9t"
   },
   "outputs": [],
   "source": [
    "path1 = \"ADHD-comment.csv\"\n",
    "path2 = \"ADHD.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0o6QiX-vkjYy"
   },
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(path1, encoding='latin1', on_bad_lines='skip', low_memory=False, nrows=1000000)\n",
    "data2 = pd.read_csv(path2, encoding='latin1', on_bad_lines='skip', low_memory=False, nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "PDQc40f4k-T1",
    "outputId": "751f0d95-b649-43ce-efe9-9e50c71ad637"
   },
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "wedpyirPk_9V",
    "outputId": "40ab4063-0e56-4175-948c-e1d3782da14f"
   },
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JemE5Z-KnVYn",
    "outputId": "e58dcf43-f1bd-4d02-8c1d-8157da6a4967"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.tokenize.punkt\n",
    "#import nltk.tokenize.stopwords\n",
    "#import nltk.tokenize.wordnet\n",
    "print(nltk.data.path)\n",
    "os.environ[\"DEFAULT_URL\"] = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml'\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "co7i6JiDnbyV"
   },
   "outputs": [],
   "source": [
    "def preprocess_comments(data, text_column='body', score_column='score', test_size=0.2, random_state=42):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    X = data[text_column]\n",
    "    Y = data[score_column]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    num_samples_train = X_train.shape[0]\n",
    "    num_samples_test = X_test.shape[0]\n",
    "\n",
    "    print(f\"num_samples_train = {num_samples_train}\")\n",
    "    print(f\"num_samples_test = {num_samples_test}\")\n",
    "\n",
    "    def preprocess_text(series):\n",
    "        tokenized = series.apply(lambda x: word_tokenize(str(x)))\n",
    "        no_stopwords = tokenized.apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation])\n",
    "        lemmatized = no_stopwords.apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
    "        preprocessed = lemmatized.apply(lambda tokens: ' '.join(tokens))\n",
    "        return preprocessed\n",
    "\n",
    "    X_train_preprocessed = preprocess_text(X_train)\n",
    "    X_test_preprocessed = preprocess_text(X_test)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_vectorized = vectorizer.transform(X_test_preprocessed)\n",
    "\n",
    "    print(X_train_vectorized.shape)\n",
    "    print(X_test_vectorized.shape)\n",
    "\n",
    "    X_combined = pd.concat([X_train_preprocessed, X_test_preprocessed])\n",
    "    X_combined_vectorized = vectorizer.fit_transform(X_combined)\n",
    "\n",
    "    print(f'Dimensions of the entire dataset: {X_combined_vectorized.shape}')\n",
    "\n",
    "    return X_train_vectorized, X_test_vectorized, y_train, y_test, X_combined_vectorized, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAf3Kxs7ng3S"
   },
   "outputs": [],
   "source": [
    "def preprocess_posts(data, text_column='selftext', score_column='score', test_size=0.2, random_state=42):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    X = data[text_column]\n",
    "    Y = data[score_column]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    num_samples_train = X_train.shape[0]\n",
    "    num_samples_test = X_test.shape[0]\n",
    "\n",
    "    print(f\"num_samples_train = {num_samples_train}\")\n",
    "    print(f\"num_samples_test = {num_samples_test}\")\n",
    "\n",
    "    def preprocess_text(series):\n",
    "        tokenized = series.apply(lambda x: word_tokenize(str(x)))\n",
    "        no_stopwords = tokenized.apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation])\n",
    "        lemmatized = no_stopwords.apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
    "        preprocessed = lemmatized.apply(lambda tokens: ' '.join(tokens))\n",
    "        return preprocessed\n",
    "\n",
    "    X_train_preprocessed = preprocess_text(X_train)\n",
    "    X_test_preprocessed = preprocess_text(X_test)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_vectorized = vectorizer.transform(X_test_preprocessed)\n",
    "\n",
    "    print(X_train_vectorized.shape)\n",
    "    print(X_test_vectorized.shape)\n",
    "\n",
    "    X_combined = pd.concat([X_train_preprocessed, X_test_preprocessed])\n",
    "    X_combined_vectorized = vectorizer.fit_transform(X_combined)\n",
    "\n",
    "    print(f'Dimensions of the entire dataset: {X_combined_vectorized.shape}')\n",
    "\n",
    "    return X_train_vectorized, X_test_vectorized, y_train, y_test, X_combined_vectorized, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjf4nLv_oJ6T"
   },
   "outputs": [],
   "source": [
    "def calculate_word_score_correlation(X_vectorized, scores, vectorizer):\n",
    "    words_df = pd.DataFrame(X_vectorized.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    if isinstance(scores, pd.Series):\n",
    "        words_df['score'] = scores.reset_index(drop=True)\n",
    "    else:\n",
    "        words_df['score'] = pd.Series(scores).reset_index(drop=True)\n",
    "    correlations = words_df.corr()['score'].sort_values(ascending=False)\n",
    "    return correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6IUSqCloP3X"
   },
   "outputs": [],
   "source": [
    "def plot_top_correlations(correlations, n=20):\n",
    "    if isinstance(correlations, pd.Series):\n",
    "        correlations = correlations.head(n)\n",
    "    else:\n",
    "        correlations = pd.Series(correlations).head(n)\n",
    "\n",
    "    top_positive_correlations = correlations[correlations > 0]\n",
    "    top_negative_correlations = correlations[correlations < 0]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x=top_positive_correlations.values, y=top_positive_correlations.index)\n",
    "    plt.title('Top Positive Word Correlations with Score')\n",
    "    plt.xlabel('Correlation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "id": "Yhw24PKXnplX",
    "outputId": "15042a75-3fa9-49a7-a355-66c240a34649"
   },
   "outputs": [],
   "source": [
    "X_train_vectorized, X_test_vectorized, y_train, y_test, X_combined_vectorized, vectorizer = preprocess_comments(data1, 'body', 'score')\n",
    "correlations = calculate_word_score_correlation(X_combined_vectorized, pd.concat([y_train, y_test]), vectorizer)\n",
    "plot_top_correlations(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "id": "BL8V9506uDTK",
    "outputId": "1163c19a-7edd-4089-ad65-33f29b110e42"
   },
   "outputs": [],
   "source": [
    "X_train_vectorized, X_test_vectorized, y_train, y_test, X_combined_vectorized, vectorizer = preprocess_posts(data2, 'selftext', 'score')\n",
    "correlations = calculate_word_score_correlation(X_combined_vectorized, pd.concat([y_train, y_test]), vectorizer)\n",
    "plot_top_correlations(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "id": "nzgFcbM3uP_C",
    "outputId": "2db7697d-e292-4d9e-97c9-938187b0f795"
   },
   "outputs": [],
   "source": [
    "X_train_vectorized, X_test_vectorized, y_train, y_test, X_combined_vectorized, vectorizer = preprocess_posts(data3, 'selftext', 'score')\n",
    "correlations = calculate_word_score_correlation(X_combined_vectorized, pd.concat([y_train, y_test]), vectorizer)\n",
    "plot_top_correlations(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "id": "RezrxPOxuSUJ",
    "outputId": "bf105c19-203e-4b67-d644-94ddbced3c86"
   },
   "outputs": [],
   "source": [
    "X_train_vectorized, X_test_vectorized, y_train, y_test, X_combined_vectorized, vectorizer = preprocess_comments(data4, 'body', 'score')\n",
    "correlations = calculate_word_score_correlation(X_combined_vectorized, pd.concat([y_train, y_test]), vectorizer)\n",
    "plot_top_correlations(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfqeERhJu1kV"
   },
   "outputs": [],
   "source": [
    "def create_word_cloud(text):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def combine_text_data(data, text_column):\n",
    "    combined_text = \" \".join(data[text_column].dropna().tolist())\n",
    "    return combined_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "rmLzouahvDKm",
    "outputId": "590d904c-0008-40e5-c994-bc0c47bb6e0a"
   },
   "outputs": [],
   "source": [
    "combined_text1 = combine_text_data(data1, 'body')\n",
    "create_word_cloud(combined_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "79_LAag_vRIR",
    "outputId": "4972f6f6-975e-4ddf-f537-9c64c4ccf20a"
   },
   "outputs": [],
   "source": [
    "combined_text2 = combine_text_data(data2, 'selftext')\n",
    "create_word_cloud(combined_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "zdydQjIPvToC",
    "outputId": "44089353-f504-4618-df1e-064aaf235e30"
   },
   "outputs": [],
   "source": [
    "combined_text3 = combine_text_data(data3, 'selftext')\n",
    "create_word_cloud(combined_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "Dinp_p8rvVwn",
    "outputId": "4d6f5b36-87af-4ed5-a19b-48704886b25f"
   },
   "outputs": [],
   "source": [
    "combined_text4 = combine_text_data(data4, 'body')\n",
    "create_word_cloud(combined_text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrHGkrVd7iLZ"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [w for w in tokens if not w.lower() in stopwords.words(\"english\")]\n",
    "    text = \" \".join(tokens)\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7C_m09Do-psS"
   },
   "outputs": [],
   "source": [
    "data1['text_cleaned'] = data1['body'].apply(lambda text: preprocess_text(text))\n",
    "data1 = data1[data1['text_cleaned'] != '']\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)\n",
    "X_tfidf = vectorizer.fit_transform(data1['text_cleaned']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IYMYyYex-rnV",
    "outputId": "843634ee-380a-4695-88c6-b5c1f26f7dcb"
   },
   "outputs": [],
   "source": [
    "def eval_cluster(embedding):\n",
    "    y_pred = kmeans.fit_predict(embedding)\n",
    "    #ari = adjusted_rand_score(data1[\"target\"], y_pred)\n",
    "    #nmi = normalized_mutual_info_score(data1[\"target\"], y_pred)\n",
    "    #fmi = fowlkes_mallows_score(data1[\"target\"], y_pred)\n",
    "\n",
    "    #print(\"Adjusted Rand Index (ARI): {:.3f}\".format(ari))\n",
    "    #print(\"Normalized Mutual Information (NMI): {:.3f}\".format(nmi))\n",
    "    #print(\"Fowlkes-Mallows Index (FMI): {:.3f}\".format(fmi))\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "kmeans.fit(X_tfidf)\n",
    "\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "data1['cluster_tfidf'] = clusters\n",
    "\n",
    "eval_cluster(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "RSlGcyIf-1Jr",
    "outputId": "097b3fe0-b9f5-4fac-9a5e-c29d06df5c87"
   },
   "outputs": [],
   "source": [
    "def dimension_reduction(embedding, method):\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    pca_vecs = pca.fit_transform(embedding)\n",
    "    data1[f'x0_{method}'] = pca_vecs[:, 0]\n",
    "    data1[f'x1_{method}'] = pca_vecs[:, 1]\n",
    "dimension_reduction(X_tfidf, 'tfidf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "id": "x-cLQfgZAP0T",
    "outputId": "c4cccdc5-aaeb-4d36-860f-b18adf7f81d2"
   },
   "outputs": [],
   "source": [
    "def plot_pca(x0_name, x1_name, cluster_name, method):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.title(f\"TF-IDF + KMeans Clustering with {method}\", fontdict={\"fontsize\": 18})\n",
    "    plt.xlabel(\"X0\", fontdict={\"fontsize\": 16})\n",
    "    plt.ylabel(\"X1\", fontdict={\"fontsize\": 16})\n",
    "    sns.scatterplot(data=data2, x=x0_name, y=x1_name, hue=cluster_name, palette=\"viridis\")\n",
    "    plt.show()\n",
    "\n",
    "plot_pca('x0_tfidf', 'x1_tfidf', 'cluster_tfidf', 'TF-IDF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 900
    },
    "id": "THWU9P3Sipm4",
    "outputId": "faefe73d-4336-4ebc-baf0-d6e5570e1378"
   },
   "outputs": [],
   "source": [
    "# Calculate WCSS for different values of k\n",
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(X_tfidf)  # Assuming X_tfidf is your tf-idf matrix\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow method\n",
    "plt.plot(range(1, 11), wcss, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfUcf2a8yAsT",
    "outputId": "6a6502e2-26bf-4f99-9d5b-034d8a0574e9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "sil_coeff = silhouette_score(X_tfidf, clusters)\n",
    "print(\"Silhouette Coefficient:\", round(sil_coeff, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtPiCwar0XZK",
    "outputId": "d9a25620-4850-4d68-b3b3-b5ed75038473"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# Calculate the Calinski-Harabasz Index\n",
    "ch_index = calinski_harabasz_score(X_tfidf, clusters)\n",
    "print(\"Calinski-Harabasz Index:\", round(ch_index, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXU18gb904vl",
    "outputId": "820a981d-2670-4c9c-9c2b-1838bc4227eb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "clusters = kmeans.labels_\n",
    "data1['cluster_tfidf'] = clusters\n",
    "dbi = davies_bouldin_score(X_tfidf, clusters)\n",
    "print(\"Davies-Bouldin Index:\", round(dbi, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fo5ATgNK-qbq"
   },
   "source": [
    "### **LDA Topic Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_3U_rWj-qL1",
    "outputId": "76ba2940-0549-4841-a595-b167113438bd"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwIPLzIeig5f"
   },
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RD4GmiOPCU69"
   },
   "outputs": [],
   "source": [
    "def preprocess_for_lda(texts):\n",
    "    # Initialize stopwords and lemmatizer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    processed_texts = []\n",
    "\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):  # Check if the input is a string\n",
    "            # Tokenize, lowercase, and filter stopwords and non-alphanumeric tokens\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "            processed_texts.append(tokens)\n",
    "        else:\n",
    "            # Handle non-string values by appending an empty list or skipping\n",
    "            processed_texts.append([])\n",
    "\n",
    "    return processed_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample_text = \"This is a simple test.\"\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(\"Tokenized sample:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yq__v-QyCvvv"
   },
   "outputs": [],
   "source": [
    "texts = data1['body'].values.tolist()\n",
    "processed_texts = preprocess_for_lda(texts)\n",
    "id2word = corpora.Dictionary(processed_texts)\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in processed_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrmIX9KPDPu0",
    "outputId": "94770543-8660-4e2a-d695-95256651af9d"
   },
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=id2word,\n",
    "                     num_topics=4,\n",
    "                     random_state=42,\n",
    "                     update_every=1,\n",
    "                     chunksize=100,\n",
    "                     passes=10,\n",
    "                     alpha='auto',\n",
    "                     per_word_topics=True)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx} \\nWords: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0QjQZ-zVhsD9",
    "outputId": "460473ca-64b6-47f1-9b39-93fa3255b88b"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHB-ErtwMbDz",
    "outputId": "fa06aec5-8c9b-4e9c-d60d-d9648571c363"
   },
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1FIAnAnPMdzW",
    "outputId": "d1cb249a-00d5-44d5-ac3d-087dfb88d1c9"
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(corpus, dictionary, texts, limit, start=2, step=3, alpha_values=None, beta_values=None):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "\n",
    "    for alpha in alpha_values:\n",
    "        for beta in beta_values:\n",
    "            for num_topics in range(start, limit, step):\n",
    "                model = LdaModel(corpus=corpus,\n",
    "                                 id2word=dictionary,\n",
    "                                 num_topics=num_topics,\n",
    "                                 random_state=100,\n",
    "                                 chunksize=100,\n",
    "                                 passes=10,\n",
    "                                 alpha=alpha,\n",
    "                                 eta=beta)\n",
    "\n",
    "                model_list.append(model)\n",
    "                coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "                coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqGL64bzMhLE",
    "outputId": "6e97c843-d38e-4e00-e2c9-4e1e29e387aa"
   },
   "outputs": [],
   "source": [
    "start = 2\n",
    "limit = 10\n",
    "step = 2\n",
    "\n",
    "alpha_values = ['auto', 'asymmetric', 0.01, 0.31]\n",
    "beta_values = ['auto', 'symmetric', 0.01, 0.31]\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(corpus=corpus,\n",
    "                                                        dictionary=id2word,\n",
    "                                                        texts=processed_texts,\n",
    "                                                        start=start,\n",
    "                                                        limit=limit,\n",
    "                                                        step=step,\n",
    "                                                        alpha_values=alpha_values,\n",
    "                                                        beta_values=beta_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60d9j6hNbmH5",
    "outputId": "fe2ea5c0-c924-47eb-d079-974ef3470ba3"
   },
   "outputs": [],
   "source": [
    "# Additional Analysis\n",
    "# Results by coherence score\n",
    "# Wordclouds for clusters\n",
    "# Coherence score for num_topic values from 1-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9LXoojgwlZJ",
    "outputId": "a439258a-ff66-4b84-ebc1-4e182d232927"
   },
   "outputs": [],
   "source": [
    "num_topics_list = []\n",
    "alpha_list = []\n",
    "beta_list = []\n",
    "coherence_score_list = []\n",
    "\n",
    "for i, (alpha, beta) in enumerate(zip(alpha_values, beta_values)):\n",
    "    for j, num_topics in enumerate(range(start, limit, step)):\n",
    "        num_topics_list.append(num_topics)\n",
    "        alpha_list.append(alpha)\n",
    "        beta_list.append(beta)\n",
    "        coherence_score_list.append(coherence_values[i * len(range(start, limit, step)) + j])\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'num_topics': num_topics_list,\n",
    "    'alpha': alpha_list,\n",
    "    'beta': beta_list,\n",
    "    'coherence_score': coherence_score_list\n",
    "})\n",
    "\n",
    "sorted_df = results_df.sort_values(by='coherence_score', ascending=False)\n",
    "sorted_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(sorted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U5t9FlP8OKt8",
    "outputId": "bab3c5ce-dca6-498a-adbc-f2be31844624"
   },
   "outputs": [],
   "source": [
    "for topic_idx in range(lda_model.num_topics):\n",
    "    words = dict(lda_model.show_topic(topic_idx, topn=20))\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(words)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"WordCloud for Topic #{topic_idx + 1}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 788
    },
    "id": "GrIA4GTQNrcx",
    "outputId": "d4cb4e1a-dc27-4a09-8229-f4a79ee5c4a9"
   },
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "topic_range = range(1, 11)\n",
    "\n",
    "coherence_scores = []\n",
    "\n",
    "for num_topics in topic_range:\n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                         id2word=id2word,\n",
    "                         num_topics=num_topics,\n",
    "                         random_state=100,\n",
    "                         chunksize=100,\n",
    "                         passes=10,\n",
    "                         alpha='auto',\n",
    "                         per_word_topics=True)\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_texts, dictionary=id2word, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "\n",
    "    coherence_scores.append(coherence_score)\n",
    "    print(f'Coherence Score for num_topics={num_topics}: {coherence_score}')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(topic_range, coherence_scores, marker='o')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.title('Coherence Score for Different Number of Topics')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adhd_data_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
